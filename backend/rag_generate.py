#!/usr/bin/env python3
"""
RAG Generation Pipeline for SIGGRAPH 2025 Papers.

Uses the retrieval pipeline to find relevant chunks,
then generates an answer using an LLM via OpenRouter API.

Usage:
    from rag_generate import RAGGenerator, GenerationConfig, SYSTEM_PROMPT
    
    generator = RAGGenerator()
    result = generator.generate("What is 3D Gaussian Splatting?")
    print(result["answer"])
"""

import os
import requests
from typing import Optional
from dataclasses import dataclass

from dotenv import load_dotenv
load_dotenv()

from retrieval_pipeline import RetrievalPipeline, RetrievalResult


# =============================================================================
# SYSTEM PROMPT - This tells the LLM how to behave
# =============================================================================
SYSTEM_PROMPT = """You are an expert research assistant specializing in computer graphics, specifically SIGGRAPH 2025 papers.

Your task is to answer questions using ONLY the provided research paper excerpts.

Rules:
1. Cite sources using [Paper Title] format
2. Be comprehensive and technically accurate
3. If the excerpts don't contain the answer, say so
4. Use LaTeX for math: $inline$ or $$block$$
5. Do NOT make up information not in the excerpts
6. Do NOT include a References section at the end
"""


# =============================================================================
# QUERY REFINEMENT PROMPT
# =============================================================================
QUERY_REFINEMENT_PROMPT = """You are an expert at refining search queries for academic paper retrieval.

Given a user's question, rewrite it as a clear, focused search query that will retrieve the most relevant research papers.

Keep it concise (under 20 words). Focus on key technical terms.

User question: {query}

Refined search query:"""


# =============================================================================
# CONFIGURATION
# =============================================================================
@dataclass
class GenerationConfig:
    """Configuration for the RAG generator."""

    llm_model: str = "gpt-4o"  # Model to use for answer generation
    temperature: float = 0.1  # Low temperature for factual answers
    max_tokens: int = 2000  # Max length of generated answer
    openrouter_api_key: Optional[str] = None  # Will load from env if not set
    refine_query: bool = True  # Whether to refine queries before retrieval
    refinement_model: str = "openai/gpt-3.5-turbo"  # Cheaper model for refinement
    retrieval_top_k: int = 8  # Number of chunks to retrieve


# =============================================================================
# RAG GENERATOR CLASS
# =============================================================================
class RAGGenerator:
    """
    Main RAG class - this is what api_server.py uses!

    Flow:
    1. Refine the user's query (optional)
    2. Retrieve relevant chunks using the retrieval pipeline
    3. Format chunks into context
    4. Generate answer using LLM
    5. Return answer with source metadata
    """

    def __init__(
        self, config: Optional[GenerationConfig] = None, retrieval_pipeline=None
    ):
        """
        Initialize the RAG generator.
        Args:
            config: Optional configuration object
            retrieval_pipeline: Optional pre-initialized retrieval pipeline
        """
        self.config = config or GenerationConfig()
        self.retrieval = retrieval_pipeline or RetrievalPipeline()
        self.openrouter_api_key = self.config.openrouter_api_key or os.getenv(
            "OPENROUTER_API_KEY"
        )
        if not self.openrouter_api_key:
            raise ValueError("OPENROUTER_API_KEY not set")
        self.openrouter_base_url = "https://openrouter.ai/api/v1"

    def refine_query(self, query: str) -> str:
        """
        Use LLM to improve the search query (optional but helps retrieval).
        Args:
            query: Original user query

        Returns:
            Refined query (or original if refinement disabled/fails)
        """
        if not self.refine_query:
            return query
        refine_query_prompt = QUERY_REFINEMENT_PROMPT.format(query=query)
        headers = {
            "Authorization": f"Bearer {self.openrouter_api_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": self.config.refinement_model,
            "messages": [{"role": "user", "content": refine_query_prompt}],
            "temperature": 0.3,
            "max_tokens": 100,
        }
        url = f"{self.openrouter_base_url}/chat/completions"
        response = requests.post(url=url, headers=headers, json=payload)
        if response.status_code != 200:
            print(
                f"[Query Refinement] Failed to refine query (status={response.status_code}): {response.text}. Falling back to original query."
            )
            return query
        response_json = response.json()
        refined = response_json["choices"][0]["message"]["content"].strip()
        refined = refined.strip("\"'")
        return refined

    def _format_context(self, results: list[RetrievalResult]) -> str:
        """
        Format retrieved chunks into a context string for the LLM.
        Args:
            results: List of RetrievalResult objects

        Returns:
            Formatted context string
        """
        return "\n".join(
            f"--- Source {i} ---\nTitle: {result.title}\nAuthors: {result.authors}\nSection: {result.chunk_section}\n\nContent:\n{result.text}"
            for i, result in enumerate(results, start=1)
        )

    def _build_sources_metadata(self, results: list[RetrievalResult]) -> list[dict]:
        """
        Build list of unique source papers for citations.
        The frontend displays these as clickable source links.
        Args:
            results: List of RetrievalResult objects

        Returns:
            List of unique source metadata dicts
        """
        seen = {}
        for result in results:
            if result.title not in seen:
                seen[result.title] = {
                    "title": result.title,
                    "authors": result.authors,
                    "pdf_url": result.pdf_url,
                    "github_link": result.github_link,
                    "video_link": result.video_link,
                    "acm_url": result.acm_url,
                    "abstract_url": result.abstract_url,
                }
        return list(seen.values())

    def _call_llm(self, query: str, context: str) -> str:
        """
        Call OpenRouter API to generate an answer.
        Args:
            query: User's question
            context: Formatted context from retrieved chunks

        Returns:
            Generated answer string
        """
        user_message = f"""Based on the following research paper excerpts, answer this question.

           Question: {query}

           Research Paper Excerpts:
           {context}

           Remember to cite papers using [Paper Title] format."""
        headers = {
            "Authorization": f"Bearer {self.openrouter_api_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": self.config.llm_model,
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_message},
            ],
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
        }
        url = f"{self.openrouter_base_url}/chat/completions"
        response = requests.post(url=url, headers=headers, json=payload)
        response_json = response.json()
        answer = response_json["choices"][0]["message"]["content"]
        return answer

    def generate(
        self, query: str, top_k: Optional[int] = None, return_sources: bool = True
    ) -> dict:
        """
        Full RAG pipeline - retrieve relevant chunks and generate an answer.
        THIS IS THE MAIN METHOD THAT api_server.py CALLS!

        TODO:
        1. Refine the query:

        2. Retrieve relevant chunks:

        3. Handle empty results:
           if not results:
               return {
                   "query": query,
                   "refined_query": refined,
                   "answer": "I couldn't find any relevant papers to answer this question.",
                   "sources": []
               }

        4. Format context from results:

        5. Generate answer using LLM:

        6. Build and return response dict:
           {
               "query": query,
               "refined_query": refined,
               "answer": answer,
               "sources": self._build_sources_metadata(results) if return_sources else []
           }

        Args:
            query: User's question
            top_k: Number of chunks to retrieve (uses config default if None)
            return_sources: Whether to include source metadata

        Returns:
            Dict with query, refined_query, answer, and sources
        """
        refined = self.refine_query(query=query)
        results = self.retrieval.retrieve(query=refined, top_k=top_k or self.config.retrieval_top_k)
        if not results:
            return {
                "query": query,
                "refined_query": refined,
                "answer": "I couldn't find any relevant papers to answer this question.",
                "sources": [],
            }
        formatted_results = self._format_context(results)
        answer = self._call_llm(refined, formatted_results)
        return {
            "query": query,
            "refined_query": refined,
            "answer": answer,
            "sources": self._build_sources_metadata(results) if return_sources else [],
        }


# =============================================================================
# CLI FOR TESTING
# =============================================================================
if __name__ == "__main__":
    import sys

    query = sys.argv[1] if len(sys.argv) > 1 else "What is 3D Gaussian Splatting?"

    print("Initializing RAG Generator...")
    generator = RAGGenerator()

    print(f"\nQuery: {query}")
    print("=" * 60)

    result = generator.generate(query)

    print(f"Refined Query: {result.get('refined_query', 'N/A')}")
    print("=" * 60)
    print("\nAnswer:")
    print(result["answer"])
    print("=" * 60)
    print(f"\nSources: {len(result.get('sources', []))} papers")
    for source in result.get("sources", []):
        print(f"  - {source['title']}")
